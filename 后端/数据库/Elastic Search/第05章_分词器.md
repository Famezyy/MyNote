# 第05章_分词器

分词器官方称为文本分析器，顾名思义是对文本进行分析处理的一种手段，基本处理逻辑为按照预先制定的分词规则，把原始文档分割成若干更小粒度的词项 - `term`，粒度大小取决于分词器规则。

例如对于下面的场景：

<img src="img/第05章_分词器/1.1.jpg" alt="img" style="zoom:80%;" />

分词器的处理过程发生在 `Index Time` 和 `Search Time` 两个时期：

- `Index Time`：文档写入并创建倒排索引时期，其分词逻辑取决于映射参数 `analyzer`
- `Search Time`：搜索发生时期，其分词仅对搜索词产生作用

分词器的组成：

- **切词器**（Tokenizer）：用于定义切词（分词）逻辑
- **词项过滤器**（Token Filter）：用于对分词之后的单个词项的处理逻辑
- **字符过滤器**（Character Filter）：用于处理单个字符

> **注意**
>
> 分词器不会对源数据造成任何影响，分词仅仅是对倒排索引或者搜索词的行为。

## 1.分词器

**分词器**同时包含切词器、词项过滤器和字符过滤器。分词器的所有行为仅仅会影响倒排索引，==不会更改元数据==。

### 1.1 文档规范化处理

为了使得搜索时可以匹配到相同意思的不同词语，**分词器**会对文档做归一化处理，例如统一大小写、转换时态、消除一些语气词和介词等，从而增加召回率和减小匹配次数，进而提高查询性能。

> **注意**
>
> 文档归一化处理的场景不仅限于以上几点，具体取决于分词器如何定义。

<img src="https://www.elastic.org.cn/upload/2023/04/2.1-1682159152776.jpg" alt="2.1-1682159152776" style="zoom: 80%;" />

ES 提供了 `_analyzer` API 用来查看指定分词器的分词结果：

```json
GET _analyze
{
    "text": ["What are you doing!"],
    "analyzer": "english"
}
```

结果

```json
{
    "tokens": [
        {
            "token": "what",
            "start_offset": 0,
            "end_offset": 4,
            "type": "<ALPHANUM>",
            "position": 0
        },
        {
            "token": "you",
            "start_offset": 9,
            "end_offset": 12,
            "type": "<ALPHANUM>",
            "position": 2
        },
        {
            "token": "do",
            "start_offset": 13,
            "end_offset": 18,
            "type": "<ALPHANUM>",
            "position": 3
        }
    ]
}
```

使用 `english` 分词器后可以发现除了将文本切成一个一个的此项后，还会进行规范化处理。

### 1.2 内置分词器

- **Standard ★**：默认分词器，中文支持的不理想，会被逐字拆分。参数值为：`standard`
- **Pattern**：以正则匹配分隔符，把文本拆分成若干词项。参数值为：`pattern`
- **Simple**：除了英文单词和字母，其他统统过滤掉，参数值为：`simple`
- **Whitespace ★**：以空白符分隔，不会改变大小写，参数值为：`whitespace`
- **Keyword ★**：可以理解为不做任何操作的分词器，会保留原有文本的所有属性，参数值为：`keyword`
- **Stop**：分词规则和 Simple Analyzer 相同，但是增加了对停用词的支持。参数值为：`stop`
- **[Language Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html)**：支持全球三十多种主流语言
- **Fingerprint**：一种特殊领域分词器，不常用

### 1.3 自定义分词器

如果 ES 内置分词器无法满足需要，可以通过对**【切词器】**、**【词项过滤器】**、**【字符过滤器】**三个组件的自由组合来自定义分词器。在使用分词器时候需要注意必须满足以下要求：

- **Tokenizer**：必须包含一个并且只能指定一个切词器，即必须指定分词器的切词规则
- **Token Filter**：可以不指定词项过滤器，也可以指定多个词项过滤器
- **Char Filter**：可以不指定字符过滤器，也可以指定多个字符过滤器

**创建语法**

```json
PUT <index_name>
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_custom_analyzer": {	     // 自定义分词器名称
                    "type": "<value>", 		 // 接受 ES 内置分词器，也可以配置为 "custom" 从而自定义分词器
                    ...
                }
            }
        }
    }
}
```

**示例**

```json
PUT test_analyzer
{
    "settings": {
        "analysis": {
            // 定义切词器
            "tokenizer": {
                "my_tokenizer": {
                    "type": "pattern",
                    "pattern": "[ ,.!?]"
                },
                "my_search_tokenizer": {
                    "type": "standard"
                },
            },
            // 定义字符过滤器
            "char_filter": {
                "html_strip_char_filter": {
                    "type": "html_strip",
                    "escaped_tags": [
                        "a"
                    ]
                },
                "my_char_filter": {
                    "type": "mapping",
                    "mappings": [
                        "滚 => *",
                        "垃 => *",
                        "圾 => *"
                    ]
                }
            },
            // 定义词项过滤器
            "filter": {
                "my_filter": {
                    "type": "stop",
                    "stopwords": [
                        "www"
                    ],
                    "ignore_case": true
                }
            },
            // 定义分词器
            "analyzer": {
                "my_analyzer": {
                    // 创建自定义分词器
                    "type": "custom",
                    // 指定创建的字符过滤器
                    "char_filter": [
                        "my_char_filter",
                        "html_strip_char_filter"
                    ],
                    // 指定创建的词项过滤器
                    "filter": [
                        "my_filter",
                        "uppercase"
                    ],
                    // 定义切词器，只能指定一个
                    "tokenizer": "my_tokenizer"
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "title":{
                "type": "text",
                "analyzer": "my_analyzer"
            }
        }
    }
}
```

`analyzer` 也可以在创建索引后指定：

```json
PUT test_analyzer/_mapping
{
    "properties": {
        "title": {
            "type": "text",
            "analyzer": "my_analyzer"
        }
    }
}
```

查看

```json
GET test_analyzer/_analyze
{
  "analyzer": "my_analyzer",
  "text": ["asd垃圾a滚sd,<a>www</a>.elastic!org?<span>cnelasticsearch</span>"]
}
```

结果

```json
{
  "tokens": [
    {
      "token": "ASD**A*SD",
      "start_offset": 0,
      "end_offset": 9,
      "type": "word",
      "position": 0
    },
    {
      "token": "<A>WWW</A>",
      "start_offset": 10,
      "end_offset": 20,
      "type": "word",
      "position": 1
    },
    {
      "token": "ELASTIC",
      "start_offset": 21,
      "end_offset": 28,
      "type": "word",
      "position": 2
    },
    {
      "token": "ORG",
      "start_offset": 29,
      "end_offset": 32,
      "type": "word",
      "position": 3
    },
    {
      "token": "CNELASTICSEARCH",
      "start_offset": 39,
      "end_offset": 70,
      "type": "word",
      "position": 4
    }
  ]
}
```

### 1.4 配置分词器映射

可通过 `mappings` 指定在 `index time` 和 `search time` 使用的分词，使用的参数如下：

- `analyzer`：为字段指定的分词器，仅对文本字段生效，针对的是源数据字段，也就是 `source data`
- `search_analyzer`：搜索时分词器，即作用于搜索词的分词器，作用对象为用户传入的搜索词

当 `search_analyzer` 未指定时，其缺省值为 `analyzer`，若 `analyzer` 未指定，`search_analyzer` 和 `analyzer` 的值都为 `standard`。

**语法**

```json
PUT test_analyzer
{
    "mappings": {
        "properties": {
            "title":{
                "type": "text",
                "analyzer": "my_analyzer",
                "search_analyzer": "my_search_analyzer"
            }
        }
    }
}
```

**示例**

为 `my_analyzer` 配置以空格 ` ` 为分割的分词器，为 `my_search_analyzer` 配置以 `,` 为分割的分词器，同时添加字符过滤器 `滚 => *` 和停用词 `*`：

```json
PUT test_analyzer
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "my_tokenizer": {
          "type": "pattern",
          "pattern": "[ ]"
        },
        "my_search_tokenizer": {
          "type": "pattern",
          "pattern": "[,]"
        }
      },
      "filter": {
        "my_filter": {
          "type": "stop",
          "stopwords": [
            "*"
          ],
          "ignore_case": true
        }
      },
      "char_filter": {
        "my_char_filter": {
          "type": "mapping",
          "mappings": [
            "滚 => *"
          ]
        }
      },
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "tokenizer": "my_tokenizer"
        },
        "my_search_analyzer": {
          "type": "custom",
          "filter": [
            "my_filter"
          ],
          "tokenizer": "my_search_tokenizer",
          "char_filter": [
            "my_char_filter"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "my_analyzer",
        "search_analyzer": "my_search_analyzer"
      }
    }
  }
}
```

添加一个文档

```json
put test_analyzer/_doc/1
{
	// 会被 my_analyzer 分割成 "1", "2", "345"
    "title": "1 2 345"
}
```

查找该文档

```json
get test_analyzer/_search
{
    "query": {
        "match": {
            // 会被 my_search_analyzer 分割成 1 45
            "title": "1,滚,45"
        }
    }
}
```

成功匹配。

### 1.5 中文分词器

IK 下载地址：https://github.com/medcl/elasticsearch-analysis-ik（下载 ES 对应的版本）

安装时首先创建插件文件夹：`cd {es-root-path}/plugins/ && mkdir ik`，将插件解压缩到文件夹 `{es-root-path}/plugins/ik`，最后重新启动 ES 服务。

#### 1.配置文件

- `IKAnalyzer.cfg.xml`：IK 分词配置文件
- `main.dic`：主词库
- `stopword.dic`：英文停用词，不会建立在倒排索引中

- `quantifier.dic`：计量单位等
- `suffix.dic`：行政单位
- `surname.dic`：姓氏
- `preposition`：语气词

#### 2.基本使用

ik 提供的两种 `analyzer`：

- `ik_max_word`：会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国，中华人民，中华，华人，人民共和国，人民，人，民，共和国，共和，和，国国，国歌”，会穷尽各种可能的组合，适合全文检索
- `ik_smart`：会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国，国歌”，适合短语（`phrase`）查询

#### 2.词库扩展

##### 2.1 本地词库

修改配置文件：IKAnalyzer.cfg.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
    <comment>IK Analyzer 扩展配置</comment>
    <!-- 配置自己的扩展字典 -->
    <entry key="ext_dict">custom/es_extend.dic;custom/es_buzzword.dic</entry>
    <!-- 配置自己的扩展停止词字典 -->
    <entry key="ext_stopwords"></entry>
    <!-- <entry key="remote_ext_dict">words_location</entry> -->
    <!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>
```

扩展词库的根目录为 `ik/config`，多个词库文件路径使用 `;` 隔开。修改后需要重启 ES。

- **优点**：配置简单，无需任何代码
- **缺点**：不支持热更新

##### 2.2 远程词库

修改配置文件：IKAnalyzer.cfg.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
    <comment>IK Analyzer 扩展配置</comment>
    <!-- <entry key="ext_dict">custom/es_extend.dic;custom/es_buzzword.dic</entry> -->
    <!-- <entry key="ext_stopwords"></entry> -->
    <!-- 配置远程扩展字典 -->
    <entry key="remote_ext_dict">http://localhost:9081/api/hotWord?wordlib=1</entry>
    <!-- 配置远程扩展停止词字典 -->
    <entry key="remote_ext_stopwords">http://localhost:9081/api/hotWord?wordlib=0</entry>
</properties>
```

字典接口

```java
@RestController
public class MyController {
    @GetMapping(value = "/api/hotWord")
    public ResponseEntity<Resource> msbHotword(int wordlib) throws IOException {
        ClassPathResource classPathResource = new ClassPathResource(wordlib == 1 ? "extra_custom.dic" : "extra_stop.dic");
        File file = classPathResource.getFile();
        return ResponseEntity
                .ok()
                .contentType(MediaType.valueOf("text/plain;charset=UTF-8"))
                .header("Last-Modified", String.valueOf(file.lastModified()))
                .header("ETag", String.valueOf(file.lastModified()))
                .body(classPathResource);
    }
}
```

- **优点**：支持热更新
- **缺点**：
  - 词库的管理不方便，要操作直接操作磁盘文件，检索也很麻烦
  - 文件的读写没有专门的优化性能不好
  - 多一层接口调用和网络传输

##### ==2.3 MySQL词库热更新==

IK 本身不支持连接数据库，需要下载并修改源码。

- 配置 MySQL 连接信息

  ```xml
  <properties>
  	<comment>IK Analyzer 扩展配置</comment>
  	<entry key="ext_dict"></entry>
  	<entry key="ext_stopwords"></entry>
      <!--配置是否启用 mysql 词库 -->
      <entry key = "enableMysql">true</entry>
      <!--配置连接信息 -->
  	<entry key = "jdbcUrl">jdbc:mysql://localhost:3306/es</entry>
  	<entry key = "jdbcUser">root</entry>
  	<entry key = "jdbcPassword">root</entry>
  	<entry key = "jdbcStopWordSql">select stop_word from ext_stop_word</entry>
  	<entry key = "jdbcWordSql">select word from ext_word</entry>
      <!-- 通过校验和判断表是否经过修改 -->
  	<entry key = "jdbcWordUpdateSql">CHECKSUM TABLE ext_word</entry>
  	<entry key = "jdbcStopWordUpdateSql">CHECKSUM TABLE ext_stop_word</entry>
  	<entry key = "jdbcReloadInterval">1</entry>
  </properties>
  ```

- 在 `org.wltea.analyzer.dic.Dictonary` 中增加连接数据库的方法

  ```java
  // 用于动态更新字典
  public void reloadMysqlDict() {
  		logger.info("start to reload ik dict.");
  		// 新开一个实例加载词典，减少加载过程对当前词典使用的影响
  		Dictionary tmpDict = new Dictionary(configuration);
  		tmpDict.configuration = getSingleton().configuration;
  		tmpDict.loadMysqlDict();
  		_MainDict = tmpDict._MainDict;
  		_StopWords = tmpDict._StopWords;
  		logger.info("reload ik dict finished.");
  	}
  
  private void loadMysqlDict() {
  		if (Boolean.FALSE.toString().equals(props.getProperty("enableMysql"))) {
  			return;
  		}
  		try (// 建立数据库连接
  			 Connection connection = DriverManager.getConnection(
  					 props.getProperty("jdbcUrl"),
  					 props.getProperty("jdbcUser"),
  					 props.getProperty("jdbcPassword")
  			 );
  			 // 创建 SQL 声明
  			 PreparedStatement statement = connection.prepareStatement(props.getProperty("jdbcWordSql"));
  			 // 创建结果集
  			 ResultSet resultSet = statement.executeQuery()) {
  			logger.info("updating local dict");
  			while (resultSet.next()) {
  				String word = resultSet.getString("word");
  				// 添加到字典中
  				_MainDict.fillSegment(word.toCharArray());
  			}
  			logger.info("finished updating local dict");
  		} catch (SQLException e) {
  			logger.error(e.getMessage());
  		}
  	}
  ```
  
- 配置动态更新

  - 新建一个 `Monitor` 用于判断表是否发生了变化

    ```java
    public class MysqlMonitor implements Runnable {
    
        private static final Logger logger = ESPluginLoggerFactory.getLogger(MysqlMonitor.class.getName());
    
        private Properties props;
        private long checkSum;
    
        public MysqlMonitor(Properties props) {
            this.props = props;
        }
    
        @Override
        public void run() {
            try (// 建立数据库连接
                Connection connection = DriverManager.getConnection(
                    props.getProperty("jdbcUrl"),
                    props.getProperty("jdbcUser"),
                    props.getProperty("jdbcPassword")
                );
                // 创建 SQL 声明
                PreparedStatement statement = connection.prepareStatement(props.getProperty("jdbcWordUpdateSql"));
                // 创建结果集
                ResultSet resultSet = statement.executeQuery()) {
                if (resultSet.next()) {
                    long new_checkSum = resultSet.getLong("Checksum");
                    // 如果更新事件发生变化则更新词库
                    if(new_checkSum != checkSum) {
                        logger.info("table got updated, start to update local dict");
                        Dictionary.getSingleton().reloadMysqlDict();
                        checkSum = new_checkSum;
                    }
                }
            } catch (SQLException e) {
                logger.error(e);
            }
        }
    }
    ```
    
  - 在 `Dictionary` 的 `initial()` 方法中增加定时任务

    ```java
    public static synchronized void initial(Configuration cfg) {
        if (singleton == null) {
            synchronized (Dictionary.class) {
                if (singleton == null) {
                    singleton = new Dictionary(cfg);
                    singleton.loadMainDict();
                    singleton.loadSurnameDict();
                    singleton.loadQuantifierDict();
                    singleton.loadSuffixDict();
                    singleton.loadPrepDict();
                    singleton.loadStopWordDict();
                    if(cfg.isEnableRemoteDict()){
                        // 建立监控线程
                        for (String location : singleton.getRemoteExtDictionarys()) {
                            pool.scheduleAtFixedRate(new Monitor(location), 10, 60, TimeUnit.SECONDS);
                        }
                        for (String location : singleton.getRemoteExtStopWordDictionarys()) {
                            pool.scheduleAtFixedRate(new Monitor(location), 10, 60, TimeUnit.SECONDS);
                        }
                        // 添加定时任务执行 Monitor
                        if (Boolean.TRUE.toString().equals(singleton.props.getProperty("enableMysql"))) {
                            pool.scheduleAtFixedRate(
                                new MysqlMonitor(singleton.props), 10, Long.parseLong(singleton.props.getProperty("jdbcReloadInterval")), TimeUnit.SECONDS);
                        }
                    }
                }
            }
        }
    }
    ```
  
- 配置 mysql 驱动

  - 添加依赖

    ```xml
    <dependency>
        <groupId>com.mysql</groupId>
        <artifactId>mysql-connector-j</artifactId>
        <version>8.3.0</version>
    </dependency>
    ```

  - 在 `Dictionary` 中加载 mysql 驱动

    ```java
    static {
        try {
            Class.forName("com.mysql.cj.jdbc.Driver");
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
    ```

  - 在 `main/assemblies/plugin.xml` 中添加配置，使得打包时会包括 mysql 驱动

    ```xml
    <dependencySet>
        <outputDirectory/>
        <useProjectArtifact>true</useProjectArtifact>
        <useTransitiveFiltering>true</useTransitiveFiltering>
        <includes>
            <include>com.mysql:mysql-connector-j</include>
        </includes>
    </dependencySet>
    ```

  - 修改 `resources/plugin-security.policy` 增加 JVM 权限使得可以在运行时加载类

    ```bash
    grant {
    	permission java.net.SocketPermission "*", "connect,resolve";
    	# 增加以下权限
    	permission java.lang.RuntimePermission "setContextClassLoader";
    };
    ```

- 打包后将 `release/xxx.zip` 文件解压到 `plugins` 中

> **问题**
>
> 1. **java.sql.SQLNonTransientConnectionException: Could not create connection to database server**
>
>    打开 ES 自带的 JDK 安全文件：
>
>    ```bash]
>    vim jdk/lib/security/default.policy
>    ```
>
>    添加内容，目的是给该 ip 和端口开通 socket 网络链接权限：
>
>    ```bash
>    grant {
>    	permission java.net.SocketPermission "192.168.244.1:3306","connect,resolve"; 
>    	# permission java.net.SocketPermission "*", "connect,resolve";
>    };
>    ```

## 2.切词器

`Tokenizer` 是分词器的核心组成部分之一，其主要作用是**分词**，或称之为切词，用来对原始文本进行细粒度拆分。拆分之后的每一个部分称之为一个 `Term`，或称之为一个词项。

可以把切词器理解为预定义的切词规则。

官方内置了很多种切词器，可通过 `tokenizer` 指定，默认的切词器是 `standard`。

```json
GET _analyze
{
    "tokenizer" : "standard",
    "text" : ["What are you doing"]
}
```

结果

```json
{
    "tokens": [
        {
            "token": "What",
            "start_offset": 0,
            "end_offset": 4,
            "type": "<ALPHANUM>",
            "position": 0
        },
        {
            "token": "are",
            "start_offset": 5,
            "end_offset": 8,
            "type": "<ALPHANUM>",
            "position": 1
        },
        {
            "token": "you",
            "start_offset": 9,
            "end_offset": 12,
            "type": "<ALPHANUM>",
            "position": 2
        },
        {
            "token": "doing",
            "start_offset": 13,
            "end_offset": 18,
            "type": "<ALPHANUM>",
            "position": 3
        }
    ]
}
```

可以看到只是对文本进行了切分。

## 3.词项过滤器

词项过滤器用来处理切词完成之后的词项，例如把大小写转换，删除停用词或同义词处理等。官方同样预置了很多词项过滤器，基本可以满足日常开发的需要，当然也是支持第三方也自行开发的。

下面通过案例演示不同词项过滤器的基本使用。

### 3.1 lowercase

将字符转换为小写。

```json
GET _analyze
{
    "filter" : ["lowercase"],
    "text" : "WWW ELASTIC ORG CN"
}
```

结果

```json
{
    "tokens": [
        {
            "token": "www elastic org cn",
            "start_offset": 0,
            "end_offset": 18,
            "type": "word",
            "position": 0
        }
    ]
}
```

### 3.2 uppercase

将字符转换为大写。

```json
GET _analyze
{
    "tokenizer" : "standard",
    "filter" : ["uppercase"],
    "text" : ["www.elastic.org.cn","www elastic org cn"]
}
```

结果

```json
{
    "tokens": [
        {
            "token": "WWW.ELASTIC.ORG.CN",
            "start_offset": 0,
            "end_offset": 18,
            "type": "<ALPHANUM>",
            "position": 0
        },
        {
            "token": "WWW",
            "start_offset": 19,
            "end_offset": 22,
            "type": "<ALPHANUM>",
            "position": 101
        },
        {
            "token": "ELASTIC",
            "start_offset": 23,
            "end_offset": 30,
            "type": "<ALPHANUM>",
            "position": 102
        },
        {
            "token": "ORG",
            "start_offset": 31,
            "end_offset": 34,
            "type": "<ALPHANUM>",
            "position": 103
        },
        {
            "token": "CN",
            "start_offset": 35,
            "end_offset": 37,
            "type": "<ALPHANUM>",
            "position": 104
        }
    ]
}
```

如前所述，加上切词器 `tokenizer` 后除了转换为大写，还会进行切分。

### 3.3 停用词

在切词完成之后，会消除停用词，默认的停用词如下：

- `english`：英文停用词，默认包含 a, an, and, are, as, at, be, but, by, for, if, in, into, is, it, no, not, of, on, or, such, that, the, their, then, there, these, they, this, to, was, will, with

- `cjk`：中日韩停用词，默认包含 a, and, are, as, at, be, but, by, for, if, in, into, is, it, no, not, of, on, or, s, such, t, that, the, their, then, there, these, they, this, to, was, will, with, www

```json
GET _analyze
{
    "tokenizer": "standard", 
    "filter": ["stop"],
    "text": ["What are you doing"]
}
```

结果

```json
{
    "tokens": [
        {
            "token": "What",
            "start_offset": 0,
            "end_offset": 4,
            "type": "<ALPHANUM>",
            "position": 0
        },
        {
            "token": "you",
            "start_offset": 9,
            "end_offset": 12,
            "type": "<ALPHANUM>",
            "position": 2
        },
        {
            "token": "doing",
            "start_offset": 13,
            "end_offset": 18,
            "type": "<ALPHANUM>",
            "position": 3
        }
    ]
}
```

可以发现 `are` 被忽视掉了。

**创建语法**

通过 `analysis/filter/<filter_name>/stopwords` 指定：

```json
PUT test_token_filter_stop
{
    "settings": {
        "analysis": {
            "filter": {
                /* 该过滤器仅在 test_token_filter_stop 索引中生效 */
                "my_filter": {
					/* 类型为 stop */
                    "type": "stop",
                    /* 配置词项 */
                    "stopwords": ["stop"],
                    "ignore_case": true
                }
            }
        }
    }
}
```

查看

```json
GET test_token_filter_stop/_analyze
{
    /* 此处需要指定使用 standard 切词器，否则会使用 normalizer */
    "tokenizer": "standard", 
    "filter": ["my_filter"], 
    "text": ["What stop are you doing"]
}
```

结果

```json
{
  "tokens": [
    {
      "token": "What",
      "start_offset": 0,
      "end_offset": 4,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "are",
      "start_offset": 10,
      "end_offset": 13,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "you",
      "start_offset": 14,
      "end_offset": 17,
      "type": "<ALPHANUM>",
      "position": 3
    },
    {
      "token": "doing",
      "start_offset": 18,
      "end_offset": 23,
      "type": "<ALPHANUM>",
      "position": 4
    }
  ]
}
```

可以发现 stop 被无视掉了。

### 3.4 同义词

同义词定义方式：

- 内联：直接在 `synonym` 内部声明规则，一旦创建索引则无法修改
- 文件：在文件中定义规则，文件相对顶级目录为 ES 的 config 文件夹，可以动态修改

同义词定义规则：

- `a, b, c => d`：这种方式，a、b、c 会被 d 代替
- `a, b, c, d`：这种方式下，a、b、c、d 是等价的

#### 1.内敛方式

**（1）替换写法**

```json
PUT test_token_filter_synonym
{
    "settings": {
        "analysis": {
            "filter": {
                "my_synonym": {
                    "type": "synonym",
                    "synonyms": [ "a, b, c => d" ] // a、b、c 会被替换为 d
                }
            }
        }
    }
}
```

查看

```json
GET test_token_filter_synonym/_analyze
{
    /* 此处需要指定使用 standard 切词器，否则会使用 normalizer */
    "tokenizer": "standard", 
    "filter": ["my_synonym"], 
    "text": ["a"] // 搜索 b 和 c 的结果也是一样
}
```

结果

```json
{
    "tokens": [
        {
            "token": "d",
            "start_offset": 0,
            "end_offset": 1,
            "type": "SYNONYM",
            "position": 0
        }
    ]
}
```

发现搜到了字符 d。

**（2）等价写法**

```json
PUT test_token_filter_synonym
{
    "settings": {
        "analysis": {
            "filter": {
                "my_synonym": {
                    "type": "synonym",
                    "synonyms": [ "a, b, c, d" ] // a、b、c、d 是等价的
                }
            }
        }
    }
}
```

查看

```json
GET test_token_filter_synonym/_analyze
{
    "tokenizer": "standard", 
    "filter": ["my_synonym"], 
    "text": ["a"] // 搜索 b、c 和 d 的结果也是一样
}
```

结果

```json
{
  "tokens": [
    {
      "token": "a",
      "start_offset": 0,
      "end_offset": 1,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "b",
      "start_offset": 0,
      "end_offset": 1,
      "type": "SYNONYM",
      "position": 0
    },
    {
      "token": "c",
      "start_offset": 0,
      "end_offset": 1,
      "type": "SYNONYM",
      "position": 0
    },
    {
      "token": "d",
      "start_offset": 0,
      "end_offset": 1,
      "type": "SYNONYM",
      "position": 0
    }
  ]
}
```

可以发现搜索到了全部的等价字符。

==**注意！**==以上查询结果是创建的倒排索引，如果用 `query` 查询的话会发现两种写法都能搜索到全部的文档：

**案例**

创建索引和映射

```json
PUT test_token_filter_synonym
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "tokenizer": "standard",
                    "filter": "my_synonym"
                }
            },
            "filter": {
                "my_synonym": {
                    "type": "synonym",
                    "synonyms": [ 
                        "a, b, c => d",
                        "e, f, g, h"
                    ]
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "title": {
                "type": "text",
                "analyzer": "my_analyzer"
            }
        }
    }
}
--
POST test_token_filter_synonym/_bulk
{"index":{}}
{"title":"a"}
{"index":{}}
{"title":"b"} 
{"index":{}}
{"title":"c"} 
{"index":{}}
{"title":"d"} 
{"index":{}}
{"title":"e"} 
{"index":{}}
{"title":"f"} 
{"index":{}}
{"title":"g"} 
{"index":{}}
{"title":"h"} 
```

使用 `query` 搜索文档

```json
GET test_token_filter_synonym/_search
{
  "query": {
    "match": {
      "title": "a"
    }
  }
}
```

结果

```json
{
  ...
  "hits": {
    ...
    "hits": [
      {
        "_index": "test_token_filter_synonym",
        "_id": "nI-jUY4BVDcczL1ArNQf",
        "_score": 0.9186288,
        "_source": {
          "title": "a"
        }
      },
      {
        "_index": "test_token_filter_synonym",
        "_id": "nY-jUY4BVDcczL1ArNQf",
        "_score": 0.9186288,
        "_source": {
          "title": "b"
        }
      },
      {
        "_index": "test_token_filter_synonym",
        "_id": "no-jUY4BVDcczL1ArNQf",
        "_score": 0.9186288,
        "_source": {
          "title": "c"
        }
      },
      {
        "_index": "test_token_filter_synonym",
        "_id": "n4-jUY4BVDcczL1ArNQf",
        "_score": 0.9186288,
        "_source": {
          "title": "d"
        }
      }
    ]
  }
}
```

#### 2.文件方式

创建索引

```json
PUT test_token_filter_synonym
{
    "settings": {
        "analysis": {
            "filter": {
                "my_synonym": {
                    "type": "synonym",
                    /* 指定文件目录 */
                    "synonyms_path": "analysis/synonym.txt"
                }
            }
        }
    }
}
```

在 `config` 下创建 `analysis/synonym.txt` 文件，定义同义词规则：		

```bash
a,b,c,d
```

可以动态修改。

## 4.字符过滤器

**==分词之前==**的预处理，用于过滤无用字符。

**创建语法**

通过 `analysis/char_filter/<char_filter_name>` 指定：

```json
PUT <index_name>
{
  "settings": {
    "analysis": {
      "char_filter": {
        "my_char_filter": {
          "type": "<char_filter_type>"
        }
      }
    }
  }
}
```

`type`：使用的字符过滤器类型名称，可配置以下值：

- `html_strip`
- `mapping`
- `pattern_replace`

### 4.1 HTML标签过滤器

字符过滤器会去除 HTML 标签和转义 HTML 元素。

创建索引

```json
PUT test_char_filter
{
    "settings": {
        "analysis": {
            "char_filter": {
                "my_char_filter": {
                    "type": "html_strip"
                }
            }
        }
    }
}
```

查看

```json
GET test_char_filter/_analyze
{
  "char_filter": ["my_char_filter"],
  "text": ["<p>I&apos;m so <a>happy</a>!</p>"]
}
```

结果

```json
{
  "tokens": [
    {
      "token": """
I'm so happy!
""",
      "start_offset": 0,
      "end_offset": 32,
      "type": "word",
      "position": 0
    }
  ]
}
```

可选参数：

- `escaped_tags`：需要保留的 html 标签

  创建索引

  ```json
  PUT test_char_filter
  {
      "settings": {
          "analysis": {
              "char_filter": {
                  "my_char_filter": {
                      "type": "html_strip",
                      "escaped_tags": "a"
                  }
              }
          }
      }
  }
  ```

  查看

  ```json
  GET test_char_filter/_analyze
  {
    "char_filter": ["my_char_filter"],
    "text": ["<p>I&apos;m so <a>happy</a>!</p>"]
  }
  ```

  结果

  ```json
  {
    "tokens": [
      {
        "token": """
  I'm so <a>happy</a>!
  """,
        "start_offset": 0,
        "end_offset": 32,
        "type": "word",
        "position": 0
      }
    ]
  }
  ```

  可以发现 a 标签保留了下来。

### 4.2 字符映射过滤器

通过定义映替换为规则，把特定字符替换为指定字符。

创建索引

```json
PUT test_html_strip_filter
{
    "settings": {
        "analysis": {
            "char_filter": {
                "my_char_filter": {
                    "type": "mapping",	        // mapping 代表使用字符映射过滤器
                    "mappings": [				// 数组中规定的字符会被等价替换为 => 指定的字符
                        "滚 => *",
                        "垃 => *",
                        "圾 => *"
                    ]
                }
            }
        }
    }
}
```

查看

```json
GET test_html_strip_filter/_analyze
{
  //"tokenizer": "standard", 
  "char_filter": ["my_char_filter"],
  "text": "你就是个垃圾！滚"
}
```

结果

```json
{
  "tokens": [
    {
      "token": "你就是个**！*",
      "start_offset": 0,
      "end_offset": 8,
      "type": "word",
      "position": 0
    }
  ]
}
```

### 4.3 正则替换过滤器

创建索引

```json
PUT text_pattern_replace_filter
{
    "settings": {
        "analysis": {
            "char_filter": {
                "my_char_filter": {
                    "type": "pattern_replace",	            // pattern_replace 代表使用正则替换过滤器			
                    "pattern": """(\d{3})\d{4}(\d{4})""",	// 正则表达式
                    "replacement": "$1****$2"
                }
            }
        }
    }
}
```

查看

```json
GET text_pattern_replace_filter/_analyze
{
    "char_filter": ["my_char_filter"],
    "text": "您的手机号是18868686688"
}
```

结果

```json
{
  "tokens": [
    {
      "token": "您的手机号是188****6688",
      "start_offset": 0,
      "end_offset": 17,
      "type": "word",
      "position": 0
    }
  ]
}
```

## 5.文档归一化器

`normalizer` 与 `analyzer` 的作用类似，都是对字段进行处理，但是不同之处在于 `normalizer` 不会对字段进行分词，也就是说 `normalizer` 没有 `tokenizer`。所以 `normalizer` 是作用于 `keyword` 类型的字段的，相当于我们需要给 `keyword` 类型字段做一个额外的处理时，比如转换为小写时就可以用到 `normalizer`。

**示例**

```json
PUT test_normalizer
{
    "settings": {
        "analysis": {
            // 定义 normalizer
            "normalizer": {
                "my_normalizer": {
                    // 指定过滤器
                    "filter": [
                        "lowercase"
                    ]
                }
            },
            // 定义 analyzer
            "analyzer": {
                "my_analyzer": {
                    // 指定过滤器
                    "filter": [
                        "uppercase"
                    ],
                    "tokenizer": "standard"
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "title": {
                "type": "keyword",
                "normalizer": "my_normalizer"
            },
            "content": {
                "type": "text",
                "analyzer": "my_analyzer"
            }
        }
    }
}
```

插入一条文档

```json
PUT test_normalizer/_doc/1
{
    "title":"ELASTIC Org cn",
    "content":"ELASTIC Org cn"
}
```

首先查找 title 字段的 `ELASTIC`：

```json
GET test_normalizer/_search
{
    "query": {
        "match": {
            "title": "elastic"
        }
    }
}
```

因为 `normalizer` 不会进行分词因此查找不到，使用 `elastic org cn` 可以找到。

再查找 content 字段：

```json
GET test_normalizer/_search
{
    "query": {
        "match": {
            "content": "elastic"
        }
    }
}
```

可以成功找到。
