# 第04章_存储卷与数据持久化

Kubernetes 也支持类似 Docker 的存储卷功能以实现短生命周期的容器应用数据的持久化，不过，其存储卷**绑定于 Pod 对象**而非容器级别，并可共享给内部的所有容器使用。

## 1.存储卷基础

Pod 本身有生命周期，其应用容器及生成的数据自身均无法独立于该生命周期之外持久存在，并且同一 Pod 中的容器可共享 PID、Network、IPC（进程间通信）和 UTS（世界协调时间）名称空间，但 Mount 和 USER 名称空间却各自独立，因而**跨容器的进程彼此间默认无法基于共享的存储空间交换文件或数据**。

### 1.1 存储卷概述

简单来说，存储卷是定义在 Pod 资源之上可被其内部的所有容器挂载的共享目录，该目录关联至宿主机或某外部的存储设备之上的存储空间，可由 Pod 内的多个容器同时挂载使用。Pod 存储卷独立于容器自身的文件系统，因而也独立于容器的生命周期，它存储的数据可于容器重启或重建后继续使用。

<img src="img/第04章_存储卷与数据持久化/image-20231108183137591.png" alt="image-20231108183137591" style="zoom:67%;" />

每个工作节点基于本地内存或目录向 Pod 提供存储空间，也能够使用借助驱动程序挂载的网络文件系统或附加的块设备，例如使用挂载至本地某路径上的 NFS 文件系统等。Kubernetes 系统具体支持的存储卷类型要取决于存储卷插件的内置定义，如图所示，不过 Kubernetes 也支持管理员基于扩展接口配置使用第三方存储。另外，Kubernetes 甚至还支持一些有着特殊功用的存储卷，例如将外部信息投射至 Pod 之中的 ConfigMap、Secret 和 Downward API 等。

<img src="img/第04章_存储卷与数据持久化/image-20231108191400274.png" alt="image-20231108191400274" style="zoom:67%;" />

存储卷隶属于 Pod 资源，且**与所属的特定 Pod 对象有着相同的生命周期**，因而通过 API Server 管理声明了存储卷资源的 Pod 对象时也会相应触发存储卷的管理操作。在具体的执行过程中，首选由调度器将该 Pod 对象绑到一个工作节点之上，若该 Pod 定义存储卷尚未被挂载，Controller Manager 中的 AD 控制器（Attach/Detach Controller）会先借助相应的存储卷插件把远程的存储设备附加到该目标节点，而由内置在 kubelet中 的 Pod 管理器（Pod Manager）触发本地的存储卷操作实现，它借助存储卷管理器（Volume  Manager）调用存储卷插件进行关联并驱动相应存储服务，并完成设备的挂载、格式化和卸载等操作。**存储卷独立于 Pod 对象中容器的生命周期**，从而使得容器重启或更新之后数据依然可用，但**删除 Pod 对象时也必将删除其存储卷**。定义 Pod 资源时，用户可在其`spec.volumes`字段中嵌套配置选定的存储卷插件。

目前，Kubernetes 支持的存储卷可简单归为以下类别，它们也各自有着不少的实现插件。

1. 临时存储卷：`emptyDir`
2. 本地存储卷：`hostPath`和`local`
3. 网络存储卷
   - 云存储：awsElasticBlockStore、gcePersistentDisk、azureDisk 和 azureFile
   - 网络文件系统：NFS、GlusterFS、CephFS 和 Cinder
   - 网络块设备：iscsi、FC、RBD 和 vSphereVolume
   - 网络存储平台：Quobyte、PortworxVolume、StorageOS 和 ScaleIO
4. 特殊存储卷：Secret、ConfigMap、DownwardAPI 和 Projected
5. 扩展支持第三方存储的存储接口（Out-of-Tree 卷插件）：CSI 和 FlexVolume

通常，这些 Kubernetes 内置提供的存储卷插件可归类为`In-Tree`类型，它们同 Kubernetes 源代码一同发布和迭代，而由存储服务商借助于 CSI 或 FlexVolume 接口扩展的独立于 Kubernetes 代码的存储卷插件则统称为`Out-Of-Tree`类型，集群管理员也可根据需要创建自定义的扩展插件，目前 CSI 是较为推荐的扩展接口。

### 1.2 配置Pod存储卷

在 Pod 中定义使用存储卷的配置由两部分组成：一部分通过`.spec.volumes`字段定义在 Pod 之上的存储卷列表，它经由特定的存储卷插件并结合特定的存储系统的访问接口进行定义；另一部分是嵌套定义在容器的`volumeMounts`字段上的存储卷挂载列表，它只能挂载当前 Pod 对象中定义的存储卷。不过，定义了存储卷的 Pod 内的容器也可以选择不挂载任何存储卷。

```yaml
spec:
  volumes:
  - name <string>  # 存储卷名称标识, 仅可使用 DNS 标签格式的字符, 在当前 Pod 中必须唯一
    VOL_TYPE <Object>          # 存储卷插件及具体的目标存储系统的相关配置
  containers:
  - name: …
    image: …
    volumeMounts:
    - name <string>             # 要挂载的存储卷的名称, 必须匹配存储卷列表中某项的定义
      mountPath <string>        # 容器文件系统上的挂载点路径
      readOnly <boolean>        # 是否挂载为只读模式, 默认为“否”
      subPath <string>          # 挂载存储卷上的一个子目录至指定的挂载点
      subPathExpr <string>      # 挂载由指定模式匹配到的存储卷的文件或目录至挂载点
      mountPropagation <string> # 挂载卷的传播模式
```

Pod配置清单中的`.spec.volumes`字段的值是一个对象列表，每个列表项定义一个存储卷，它由存储卷名称（`.spec.volumes.name <String>`）和存储卷对象（`.spec.volumes.VOL_TYPE <Object>`）组成，其中`VOL_TYPE`是使用的存储卷类型名称，它的内嵌字段随类型的不同而不同，具体参数需要参阅 Pod 上各存储卷插件的相关文档说明。

定义好的存储卷可由当前 Pod 资源内的各容器进行挂载。Pod 中仅有一个容器时，使用存储卷的目的通常在于数据持久化，以免重启时导致数据丢失，当多个容器挂载同一个存储卷时“共享”才有了具体的意义。挂载卷的**传播模式**（`mountPropagation`）就是用于配置容器将其挂载卷上的数据变动传播给同一 Pod 中的其他容器，甚至是传播给同一个节点上的其他 Pod 的一个特性，该字段的可用值包括如下几项：

- `None`：该挂载卷不支持传播机制，当前容器不向其他容器或 Pod 传播自己的挂载操作，也不会感知主机后续在该挂载卷或其任何子目录上执行的挂载变动；此为默认值
- `HostToContainer`：主机向容器的单向传播，即当前容器能感知主机后续对该挂载卷或其任何子目录上执行的挂载变动
- `Bidirectional`：主机和容器间的双向传播，当前容器创建的存储卷挂载操作会传播给主机及使用了同一存储卷的所有 Pod 的所有容器，也能感知主机上后续对该挂载卷或其任何子目录上执行的挂载变动；该行为存在破坏主机操作系统的危险，因而仅可用于**特权模式**下的容器中

## 2.临时存储卷

Kubernetes 支持的存储卷类型中，`emptyDir`存储卷的生命周期与其所属的 Pod 对象相同，它无法脱离 Pod 对象的生命周期提供数据存储功能，因此通常仅用于**数据缓存**或**临时存储**。不过，基于`emptyDir`构建的`gitRepo`存储卷可以在 Pod 对象的生命周期起始时，从相应的 Git 仓库中克隆相应的数据文件到底层的`emptyDir`中，也就使得它具有了一定意义上的持久性。

### 2.1 emptyDir存储卷

`emptyDir`存储卷可以理解为 Pod 对象上的一个**临时目录**，类似于 Docker 上的“Docker 挂载卷”，在 Pod 对象启动时即被创建，而在 Pod 对象被移除时一并被删除。因此，`emptyDir`存储卷只能用于某些特殊场景中，例如同一 Pod 内的多个容器间的**文件共享**，或作为容器数据的临时存储目录用于**数据缓存系统**等。

`emptyDir`存储卷嵌套定义在`.spec.volumes.emptyDir`字段中，可用字段主要有两个：

- `medium`：此目录所在的存储介质的类型，可用值为`default`或`Memory`，默认为`default`，表示使用节点的默认存储介质；`Memory`表示使用基于 RAM 的临时文件系统 tmpfs，总体可用空间受限于内存，但性能非常好，通常用于为容器中的应用提供**缓存**存储
- `sizeLimit`：当前存储卷的空间限额，默认值为`nil`，表示不限制；不过，在`medium`字段值为`Memory`时，建议务必定义此限额

下面是一个使用了`emptyDir`存储卷的简单示例：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volumes-emptydir-demo
  namespace: default
spec:
  initContainers:
  - name: config-file-downloader
    image: ikubernetes/admin-box
    imagePullPolicy: IfNotPresent
    command: ["/bin/sh", "-c", "wget -O /data/envoy.yaml https://raw.githubusercontent.com/iKubernetes/Kubernetes_Advanced_Practical_2rd/master/chapter4/envoy.yaml"]
    volumeMounts:
    - name: config-file-store
      mountPath: /data
  containers:
  - name: envoy
    image: envoyproxy/envoy-alpine:v1.13.1
    command: ["/bin/sh", "-c"]
    args: ["envoy -c /etc/envoy/envoy.yaml"]
    volumeMounts:
    - name: config-file-store
      mountPath: /etc/envoy
      readOnly: true
  volumes:
  - name: config-file-store
    emptyDir:
      medium: Memory
      sizeLimit: 16Mi
```

在该示例清单中，为 Pod 对象定义了一个名为 config-file-store 的、基于`emptyDir`存储插件的存储卷。初始化容器将该存储卷挂载至 /data 目录后，下载 envoy.yaml 配置文件并保存于该挂载点目录下。主容器将该存储卷挂载至 /etc/envoy 目录，再通过自定义命令让容器应用在启动时加载的配置文件 /etc/envoy/envoy.yaml 上。

<img src="img/第04章_存储卷与数据持久化/image-20231108222018439.png" alt="image-20231108222018439" style="zoom: 67%;" />

Pod 资源的详细信息中会显示存储卷的相关状态，包括其是否创建成功（`Events`字段中输出）、相关的类型及参数（`Volumes`字段中输出），以及容器中的挂载状态等信息（`Containers`字段中输出）。

```bash
root@VM-0-4-ubuntu:~# kubectl describe pods volumes-emptydir-demo
...
Init Containers:
  config-file-downloader:
    Mounts:
      /data from config-file-store (rw)
...
Containers:
  envoy:
    Mounts:
      /etc/envoy from config-file-store (ro)
...
Volumes:
  config-file-store:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  16Mi
...
```

为 Envoy 下载的配置文件中定义了一个监听所有可用 IP 地址上 TCP 80 端口的 Ingress 侦听器，以及一个监听所有可用 IP 地址上 TCP 的 9901 端口的 Admin 接口。这与 Envoy 镜像中默认配置文件中的定义均有不同（默认的 Ingress 监听端口是 10000）。

```bash
root@VM-0-4-ubuntu:~# kubectl exec volumes-emptydir-demo -- netstat -tnl
Defaulted container "envoy" out of: envoy, config-file-downloader (init)
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      
tcp        0      0 0.0.0.0:9901            0.0.0.0:*               LISTEN 
```

`emptyDir`卷简单易用，但仅能用于临时存储。

### 2.2 gitRepo存储卷（deprecated）

`gitRepo`存储卷可以看作是`emptyDir`存储卷的一种实际应用，使用该存储卷的 Pod 资源可以通过挂载目录**访问指定的代码仓库中的数据**。使用`gitRepo`存储卷的 Pod 资源在创建时，会首先创建一个空目录（`emptyDir`）并克隆一份指定的 Git 仓库中的数据至该目录，而后再创建容器并挂载该存储卷。

定义`gitRepo`类型的存储卷时，其可嵌套使用字段有如下 3 个：

- `repository <string>`：Git 仓库的 URL，必选字段
- `directory <string>`：目标目录名称，但名称中不能包含`..`字符；`.`表示将仓库中的数据直接克隆至存储卷映射的目录中，其他字符则表示将数据克隆至存储卷上以用户指定的字符串为名称的子目录中
- `revision <string>`：特定 revision 的提交哈希码，会执行`git checkout <revision>`的操作，因此也可以是 branch 名

> **注意**
>
> 使用`gitRepo`存储卷的 Pod 资源运行的工作节点上必须安装有 Git 程序，否则克隆仓库的操作将无法完成。

下面的配置清单示例中的 Pod 资源在创建时，会先创建一个空目录，将指定的Git仓库`https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git`中的数据克隆一份直接保存在此目录中，而后将此目录创建为存储卷 html，再由容器 nginx 将此存储卷挂载到 /usr/share/nginx/html 目录上。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volumes-gitrepo-demo
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
  volumes:
  - name: html
    gitRepo:
      repository: https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git
      directory: .
      revision: "main"
```

访问此 Pod 资源中的 nginx 服务，即可看到它来自 Git 仓库中的页面资源。不过，`gitRepo`存储卷在其创建完成后**不会再与指定的仓库执行同步操作**。此时可以为 Pod 资源创建一个`Sidecar`容器来执行此类的同步操作，尤其是数据来源于私有仓库时，通过`Sidecar`容器完成认证等必要步骤后再进行克隆操作就更为必要。

`gitRepo`存储卷构建于`emptyDir`之上，其生命周期与 Pod 资源一样，故使用中不应在此类存储卷中保存由容器生成的重要数据。另外，`gitRepo`存储插件==即将废弃==，建议在初始化容器或`Sidecar`容器中运行`git`命令来完成相应的功能。

## 3.hostPath存储卷

`hostPath`存储卷插件是将**工作节点**上某文件系统的目录或文件关联到 Pod 上的一种存储卷类型，其数据具有**同工作节点生命周期一样**的持久性。配置`hostPath`存储卷的嵌套字段有两个：一个用于指定工作节点上的目录路径的必选字段`path`；另一个用于指定节点之上存储类型的`type`。`hostPath`支持使用的节点存储类型有如下几种：

- `DirectoryOrCreate`：指定的路径不存在时，自动将其创建为 0755 权限的空目录，属主和属组均为 kubelet
- `Directory`：事先必须存在的目录路径
- `FileOrCreate`：指定的路径不存在时，自动将其创建为 0644 权限的空文件，属主和属组均为 kubelet
- `File`：事先必须存在的文件路径
- `Socket`：事先必须存在的 Socket 文件路径
- `CharDevice`：事先必须存在的字符设备文件路径
- `BlockDevice`：事先必须存在的块设备文件路径
- `""`：空字符串，默认配置，在关联`hostPath`存储卷之前不进行任何检查

这类 Pod 对象通常受控于`DaemonSet`类型的 Pod 控制器，它运行在集群中的每个工作节点上，负责收集工作节点上系统级的相关数据，然而在节点上创建的文件或目录**默认仅 root 用户可写**，若期望容器内的进程拥有写权限，则需要将该容器运行于**特权模式**，不过这存在潜在的安全风险。

下面是定义在配置清单中的 Pod 对象，容器中的 filebeat 进程负责收集工作节点及容器相关的日志信息并发往 Redis 服务器，它使用了 3 个`hostPath`类型的存储卷，第一个指向了宿主机的日志文件目录 /var/logs，后面两个则与宿主机上的容器运行时环境有关。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: vol-hostpath-pod
spec:
  containers:
  - name: filebeat
    image: ikubernetes/filebeat:5.6.7-alpine
    env:
    - name: REDIS_HOST
      value: redis.ilinux.io:6379
    - name: LOG_LEVEL
      value: info
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: socket
      mountPath: /run/containerd/containerd.sock
    - name: varlibcontainers
      mountPath: /var/lib/containerd
      readOnly: true
  terminationGracePeriodSeconds: 30
  volumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: socket
    hostPath:
      path: /run/containerd/containerd.sock
  - name: varlibcontainers
    hostPath:
      path: /var/lib/containerd
```

上面配置清单中 Pod 对象的正确运行要依赖于 REDIS_HOST 和 LOG_LEVEL 环境变量，它们分别用于定义日志缓冲队列服务和日志级别。如果有可用的 Redis 服务器，我们就可通过环境变量 REDIS_HOST 将其对应的主机名或I P 地址传递给 Pod 对象，待 Pod 对象准备好之后即可通过 Redis 服务器查看到由该 Pod 发送的日志信息。测试时，我们仅需要给 REDIS_HOST 环境变量传递一个任意值（例如清单中的 redis.ilinux.io）便可直接创建 Pod 对象，只不过该 Pod 中容器的日志会报出无法解析指定主机名的错误，但这并不影响存储卷的配置和使用。

> **提示**
>
> 在 Filebeat 的应用架构中，这些日志信息可能会由 Logstash 收集后发往集中式日志存储系统 Elasticsearch，并通过 Kibana 进行展示。

对于由 Deployment 或 StatefulSet 等一类控制器管控的、使用了`hostPath`存储卷的 Pod 对象来说，需要注意**在 Pod 对象被重新调度至其他节点时，容器进程此前创建的文件或目录则大多不会存在**。一个常用的解决办法是通过在 Pod 对象上使用`nodeSelector`或者`nodeAffinity`赋予该 Pod 对象指定要绑定到的具体节点来影响调度器的决策，但即便如此，管理员仍然不得不手动管理涉及的多个节点之上的目录，低效且易错。因此，`hostPath`存储卷虽然能持久保存数据，但对于由调度器按需调度的应用来说并不适用。

## 4.网络存储卷

Kubernetes 内置了多种类型的网络存储卷插件，它们支持的存储服务包括传统的 NAS 或 SAN 设备（例如 NFS、iscsi 和 FC 等）、分布式存储（例如 GlusterFS、CephFS 和 RBD 等）、云存储（例如 gcePersistentDisk、azureDisk、Cinder 和 awsElasticBlockStore 等）以及构建在各类存储系统之上的抽象管理层（例如 flocker、portworxVolume 和 vSphereVolume 等）。这类服务通常都是独立运行的存储系统，因相应的存储卷可以支持超越节点生命周期的数据持久性。

### 4.1 NFS存储卷

NFS 即**网络文件系统**（Network File System），它是一种分布式文件系统协议，最初是由 Sun Microsystems 公司开发的类 UNIX 操作系统之上的经典网络存储方案，其功能旨在允许客户端主机可以像访问本地存储一样通过网络访问服务器端文件。Kubernetes 的 NFS 存储卷用于关联某事先存在的 NFS 服务器上导出的存储空间到 Pod 对象中以供容器使用，该类型的存储卷在 Pod 对象终止后仅是**被卸载**而非被删除。而且，NFS 是文件系统级共享服务，它支持同时存在的多路挂载请求，可由多个 Pod 对象同时关联使用。定义 NFS 存储卷时支持嵌套使用以下几个字段。

- `server <string>`：NFS 服务器的 IP 地址或主机名，必选字段
- `path <string>`：NFS 服务器导出（共享）的文件系统路径，必选字段
- `readOnly <boolean>`：是否以只读方式挂载，默认为 false

下面的配置清单示例中以 Redis 为例来说明 NFS 存储卷的功能与用法。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volumes-nfs-demo
  labels:
    app: redis
spec:
  containers:
  - name: redis
    image: redis:alpine
    ports:
    - containerPort: 6379
      name: redisport
    securityContext:
      runAsUser: 777
    volumeMounts:
    - mountPath: /data
      name: redisdata
  volumes:
  - name: redisdata
    nfs:
      server: localhost
      path: /data/redis
      readOnly: false
```

镜像文件会默认以 redis 用户（UID：777）运行 redis-server 进程，并将数据持久保存在容器文件系统上的 /data 目录中，因此需要确保 UID 777 的用户有权限读写该目录。同时 NFS 服务器上用于该 Pod 对象的存储卷的导出目录（/data/redis）也需要确保让 UID 为 999 的用户拥有读写权限，因此需要在 nfs.ilinux.io 服务器上创建该用户，将该用户设置为 /data/redis 目录的属主，或通过 facl 设置该用户拥有读写权限。

以 Ubuntu 为例，在一个服务器上以 root 用户设定所需的 NFS 服务器的步骤如下：

1. 安装 NFS Server 程序包
   
   ```bash
   apt -y install nfs-kernel-server
   ```

2. 设定基础环境
   
   ```bash
   mkdir /data/redis
   useradd -u 777 redis
   chown redis /data/redis
   ```

3. 编辑 /etc/exports 配置文件
   
   ```bash
   /data/redis 10.0.0.0/16(rw,no_root_squash) 10.244.0.0/16(rw,no_root_squash) 127.0.0.1(rw,no_root_squash)
   ```

   10.0.0.0/16 和 10.244.0.0/16、127.0.0.1 是允许挂载的网络网段和地址，分别是 Master 的内网网段和 Pod 的网段；而 (rw,no_root_squash) 部分表示允许以读写方式挂载，并且不对 root 用户进行权限限制。
  
4. 启动 NFS 服务器
   
   ```bash
   systemctl start nfs-server
   ```

5. 在各工作节点安装 NFS 服务客户端程序包
   
   ```bash
   apt install -y nfs-common
   ```
   
执行完成后，切换回 kubernetes 主机上运行创建 Pod 对象：

```bash
kubectl apply -f volumes-nfs-demo.yml
```

资源创建完成后，可通过命令客户端 redis-cli 创建测试数据，并手动触发与存储系统同步：

```bash
kubectl exec -it volumes-nfs-demo -- redis-cli
127.0.0.1:6379> set mykey "hello redis"
OK
127.0.0.1:6379> get mykey
"hello redis"
127.0.0.1:6379> BGSAVE
Background saving started
127.0.0.1:6379> exit
```

为了测试数据持久化效果，下面先删除并重新创建 Pod 对象：

```bash
kubectl delete -f volumes-nfs-demo.yaml
kubectl apply -f volumes-nfs-demo.yaml
```

然后进入容器后观察是否还保存有此前存储的数据：

```bash
kubectl exec -it volumes-nfs-demo -- redis-cli
127.0.0.1:6379> get mykey
"hello redis"
```

可以观察到重新创建 Pod 后键值依然存在。若需要在删除 Pod 后清除存储设备上的数据则需要用户通过存储系统的管理接口手动进行。

### 4.2 RBD存储卷

Ceph 是一个专注于分布式的、弹性可扩展的、高可靠的、性能优异的存储系统平台，同时支持提供块设备、文件系统和对象存储 3 种存储接口。

### 4.3 CephFS存储卷

### 4.4 GlusterFS存储卷

## 5.持久存储卷

### 5.1 PV与PVC基础

### 5.2 PV的生命周期

### 5.3 静态PV资源

### 5.4 PVC资源

### 5.5 在Pod中使用PVC

## 6.容器存储接口CSI

### 6.1 CSI基础

### 6.2 Longhorn存储系统

## 7.容器化应用配置

## 8.应用程序配置管理与ConfigMap资源

## 9.Secret资源：向容器注入配置信息

## 10.应用Download API存储卷配置信息